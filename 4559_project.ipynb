{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1dQ_ssdiaxDnJukjVN2crJ-iM309tMlWk","authorship_tag":"ABX9TyMoXaLFIgPc5TQENY0287IZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Face_Mask_detection"],"metadata":{"id":"75QRGOQthpVD"}},{"cell_type":"markdown","source":["Introduction:\n","\n","In today's world, the importance of face mask detection cannot be overstated. To address this pressing concern, this notebook harnesses the power of the TensorFlow 2 Object Detection API to train a state-of-the-art model. Specifically, we explore the training of an SSD-MobileNet or an EfficientDet model using a custom dataset. The end goal of this project is to not only train a highly accurate model but also to convert it to TensorFlow Lite format. Through this Colab, you will embark on a journey to create and obtain a TensorFlow Lite model, which can be seamlessly deployed on your personal computer, Android phone, or even on compact edge devices like the Raspberry Pi. This project empowers you to contribute to public health and safety by detecting the presence of face masks with precision and convenience."],"metadata":{"id":"BBpMr63sVwFF"}},{"cell_type":"markdown","source":["I obtained my dataset from Kaggle, which comprises images depicting individuals both with and without face masks. Now, let's move on to the crucial step of gathering and labeling the training images. For a preliminary model to prove the concept, we recommend starting with approximately 7k images. These training images should exhibit a mix of random objects alongside the objects of interest, i.e., faces with and without masks. Furthermore, it's important to ensure that the images encompass a diverse range of backgrounds and lighting conditions to enhance the model's robustness.\n","\n","For comprehensive guidance on the process of gathering and labeling images for training an object detection model, please watch the instructional YouTube video provided below, which offers valuable insights and tips to assist you in this endeavor."],"metadata":{"id":"VGwmPoiuWRdO"}},{"cell_type":"markdown","source":["# Step 2: Mount Google Drive"],"metadata":{"id":"dwA81Z8hSacx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mI2TnCaYSXXV","executionInfo":{"status":"ok","timestamp":1698891826266,"user_tz":240,"elapsed":1289,"user":{"displayName":"Fortune Chukwunyere Ogulewe","userId":"05672397235833648324"}},"outputId":"301413b8-b727-4235-f181-66e12c99bc09"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Step 3: Build the CNN model"],"metadata":{"id":"dmnQGaaGTG8_"}},{"cell_type":"markdown","source":["#Install TensorFlow Object Detection Dependencies"],"metadata":{"id":"f-fDm1bQW-i6"}},{"cell_type":"markdown","source":["I installed the TensorFlow Object Detection API in this Google Colab instance. This requires cloning the TensorFlow models repository and running a couple installation commands. Click the play button to run the following sections of code."],"metadata":{"id":"dLUUxNMQXAvN"}},{"cell_type":"code","source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy\n"],"metadata":{"id":"Dmk8Tev5a156","executionInfo":{"status":"ok","timestamp":1698891826267,"user_tz":240,"elapsed":9,"user":{"displayName":"Fortune Chukwunyere Ogulewe","userId":"05672397235833648324"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n","model.add(MaxPooling2D(2, 2))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(2, 2))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(2, 2))\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"],"metadata":{"id":"-x_m1uihTJDd","executionInfo":{"status":"ok","timestamp":1698891826267,"user_tz":240,"elapsed":5,"user":{"displayName":"Fortune Chukwunyere Ogulewe","userId":"05672397235833648324"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Train the model"],"metadata":{"id":"sJGgbyjATSFF"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale=1.0/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2,\n","                                   shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n","\n","train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/4559 Project/data',\n","                                                    target_size=(150, 150),\n","                                                    batch_size=32,\n","                                                    class_mode='binary')\n","\n","model.fit(train_generator, epochs=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EauHPGyNTVCh","executionInfo":{"status":"ok","timestamp":1698892467451,"user_tz":240,"elapsed":641188,"user":{"displayName":"Fortune Chukwunyere Ogulewe","userId":"05672397235833648324"}},"outputId":"eccbe188-7a05-46f5-c608-3b3f1f7e02c6"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5815 images belonging to 2 classes.\n","Epoch 1/10\n","182/182 [==============================] - 55s 286ms/step - loss: 0.4906 - accuracy: 0.7929\n","Epoch 2/10\n","182/182 [==============================] - 51s 281ms/step - loss: 0.3703 - accuracy: 0.8437\n","Epoch 3/10\n","182/182 [==============================] - 51s 279ms/step - loss: 0.3554 - accuracy: 0.8609\n","Epoch 4/10\n","182/182 [==============================] - 52s 285ms/step - loss: 0.3511 - accuracy: 0.8607\n","Epoch 5/10\n","182/182 [==============================] - 51s 282ms/step - loss: 0.3215 - accuracy: 0.8738\n","Epoch 6/10\n","182/182 [==============================] - 51s 277ms/step - loss: 0.2973 - accuracy: 0.8834\n","Epoch 7/10\n","182/182 [==============================] - 52s 284ms/step - loss: 0.2884 - accuracy: 0.8867\n","Epoch 8/10\n","182/182 [==============================] - 52s 283ms/step - loss: 0.2622 - accuracy: 0.8905\n","Epoch 9/10\n","182/182 [==============================] - 51s 280ms/step - loss: 0.2615 - accuracy: 0.8963\n","Epoch 10/10\n","182/182 [==============================] - 52s 286ms/step - loss: 0.2564 - accuracy: 0.8973\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x796522190760>"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["# Step 5: Real-time face mask detection using OpenCV"],"metadata":{"id":"MxOkgegmTfpu"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","def detect_face_mask(frame):\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","    for (x, y, w, h) in faces:\n","        face = frame[y:y+h, x:x+w]\n","        face = cv2.resize(face, (150, 150))\n","        face = np.expand_dims(face, axis=0)\n","        face = face / 255.0\n","        prediction = model.predict(face)\n","\n","        if prediction > 0.5:\n","            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n","            cv2.putText(frame, 'With Mask', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","        else:\n","            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n","            cv2.putText(frame, 'Without Mask', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n","\n","    return frame"],"metadata":{"id":"2zv7qzdKTkSb","executionInfo":{"status":"ok","timestamp":1698892467451,"user_tz":240,"elapsed":12,"user":{"displayName":"Fortune Chukwunyere Ogulewe","userId":"05672397235833648324"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["# open the webcam and start face mask detection"],"metadata":{"id":"YjOeE9tjTpQY"}},{"cell_type":"code","source":["cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    frame = detect_face_mask(frame)\n","    cv2.imshow('Face Mask Detection', frame)\n","\n","    if cv2.waitKey(1) & 0xFF == 27:  # Press 'Esc' key to exit\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"-dBUYsI1TsSu","executionInfo":{"status":"ok","timestamp":1698892742705,"user_tz":240,"elapsed":122,"user":{"displayName":"Fortune Chukwunyere Ogulewe","userId":"05672397235833648324"}}},"execution_count":53,"outputs":[]}]}